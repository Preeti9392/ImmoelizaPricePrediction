{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc6599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def clean_real_estate_data(df):\n",
    "    print(\" Starting data cleaning...\")\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "\n",
    "    # Step 1: Remove completely empty rows and columns\n",
    "    print(\"\\n Step 1: Removing empty rows and columns...\")\n",
    "\n",
    "    # Remove  Url row\n",
    "    df = df.drop(\"url\", axis=\"columns\")\n",
    "    # Remove rows where all values are NaN\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    # Remove columns where all values are NaN\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    print(f\"After removing empty: {df.shape}\")\n",
    "\n",
    "    # Step 2: Handle the unnamed index column\n",
    "    print(\"\\n Step 2: Cleaning column names...\")\n",
    "\n",
    "    # Drop the unnamed index column if it exists\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "        print(\"Removed 'Unnamed: 0' column\")\n",
    "\n",
    "    # Remove specified columns based on decisions\n",
    "    print(\"\\n Step 2.1: Removing specified columns...\")\n",
    "    columns_to_remove = [\n",
    "        'heatingType',\n",
    "        'hasThermicPanels', \n",
    "        'gardenOrientation', \n",
    "        'terraceOrientation',\n",
    "        'roomCount', \n",
    "        'monthlyCost', \n",
    "        'diningRoomSurface', \n",
    "        'streetFacadeWidth', \n",
    "        'kitchenSurface', \n",
    "        'hasBalcony', \n",
    "        'accessibleDisabledPeople'\n",
    "    ]\n",
    "    \n",
    "    columns_removed = []\n",
    "    for col in columns_to_remove:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis=1)\n",
    "            columns_removed.append(col)\n",
    "    \n",
    "    if columns_removed:\n",
    "        print(f\"Removed specified columns: {columns_removed}\")\n",
    "    else:\n",
    "        print(\"None of the specified columns were found in the dataset\")\n",
    "\n",
    "    # NEW: Merge parking columns\n",
    "    print(\"\\n Step 2.2: Merging parking columns...\")\n",
    "    if 'parkingCountOutdoor' in df.columns and 'parkingCountIndoor' in df.columns:\n",
    "        # Fill NaN values with 0 before merging\n",
    "        df['parkingCountOutdoor'] = df['parkingCountOutdoor'].fillna(0)\n",
    "        df['parkingCountIndoor'] = df['parkingCountIndoor'].fillna(0)\n",
    "        \n",
    "        # Create merged parking count\n",
    "        df['totalParkingCount'] = df['parkingCountOutdoor'] + df['parkingCountIndoor']\n",
    "        \n",
    "        # Drop original parking columns\n",
    "        df = df.drop(['parkingCountOutdoor', 'parkingCountIndoor'], axis=1)\n",
    "        print(f\"Merged parking columns into 'totalParkingCount': {df['totalParkingCount'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"Parking columns not found for merging\")\n",
    "\n",
    "    # Step 3: Remove duplicates based on ID\n",
    "    print(\"\\n Step 3: Removing duplicate properties...\")\n",
    "\n",
    "    if \"id\" in df.columns:\n",
    "        before_dup = len(df)\n",
    "        df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "        after_dup = len(df)\n",
    "        print(f\"Removed {before_dup - after_dup} duplicate properties\")\n",
    "\n",
    "    # Step 4: Clean price column (main target variable)\n",
    "    print(\"\\n Step 4: Cleaning price data...\")\n",
    "\n",
    "    if \"price\" in df.columns:\n",
    "        # Remove rows with missing prices (can't train without target)\n",
    "        before_price = len(df)\n",
    "        df = df.dropna(subset=[\"price\"])\n",
    "        after_price = len(df)\n",
    "        print(f\"Removed {before_price - after_price} rows with missing prices\")\n",
    "\n",
    "        # Remove unrealistic prices (too low or too high)\n",
    "        # Assuming Belgian real estate: min 50k, max 5M euros\n",
    "        #df = df[(df[\"price\"] >= 50000) & (df[\"price\"] <= 5000000)]\n",
    "        #print(f\"Kept prices between 50k and 5M euros: {len(df)} properties\")\n",
    "\n",
    "    # NEW: Handle garden surface imputation\n",
    "    print(\"\\n Step 4.1: Handling garden surface imputation...\")\n",
    "    if 'gardenSurface' in df.columns and 'hasGarden' in df.columns:\n",
    "        # Convert hasGarden to boolean if it's not already\n",
    "        df['hasGarden'] = df['hasGarden'].map({True: True, False: False, 'True': True, 'False': False})\n",
    "        \n",
    "        # Impute gardenSurface with 0 where hasGarden is False\n",
    "        mask = df['hasGarden'] == False\n",
    "        before_impute = df.loc[mask, 'gardenSurface'].isna().sum()\n",
    "        df.loc[mask, 'gardenSurface'] = df.loc[mask, 'gardenSurface'].fillna(0)\n",
    "        after_impute = df.loc[mask, 'gardenSurface'].isna().sum()\n",
    "        \n",
    "        print(f\"Imputed {before_impute - after_impute} gardenSurface values with 0 where hasGarden=False\")\n",
    "        print(f\"Garden surface stats: min={df['gardenSurface'].min()}, max={df['gardenSurface'].max()}, median={df['gardenSurface'].median()}\")\n",
    "    else:\n",
    "        print(\"gardenSurface or hasGarden columns not found\")\n",
    "\n",
    "    # Step 5: Clean numeric columns\n",
    "    print(\"\\n Step 5: Cleaning numeric features...\")\n",
    "\n",
    "    numeric_cols = [\"bedroomCount\", \"bathroomCount\", \"toiletCount\", \"terraceSurface\", \"totalParkingCount\"]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # Replace negative values with NaN\n",
    "            df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "            # Replace unrealistic high values\n",
    "            if col in [\"bedroomCount\", \"bathroomCount\"]:\n",
    "                df.loc[df[col] > 10, col] = np.nan  # Max 10 bedrooms/bathrooms\n",
    "            elif col == \"toiletCount\":\n",
    "                df.loc[df[col] > 5, col] = np.nan  # Max 5 toilets                               ############################################\n",
    "            elif col == \"totalParkingCount\":\n",
    "                df.loc[df[col] > 10, col] = np.nan  # Max 10 parking spaces\n",
    "\n",
    "            print(f\"Cleaned {col}: {df[col].notna().sum()} valid values\")\n",
    "    # facedecount can be 4 max\n",
    "    if \"facedeCount\" in df.columns:\n",
    "        facedes={1.0:1,2.0:2,3.0:3,4.0:4,5.0:4,6.0:4,7.0:4,8.0:4,9.0:4,10.0:4,16.0:4,86.0:4}\n",
    "        df[\"facedeCount\"]=df[\"facedeCount\"].map(facedes)\n",
    "\n",
    "    # Step 6: Clean categorical columns\n",
    "    print(\"\\n Step 6: Cleaning categorical features...\")\n",
    "\n",
    "    # Clean property type and subtype - REMOVE ALL SPACES\n",
    "    if \"type\" in df.columns:\n",
    "        df[\"type\"] = df[\"type\"].str.upper().str.strip().str.replace(\" \", \"\")\n",
    "        print(f\"Property types: {df['type'].value_counts().to_dict()}\")\n",
    "\n",
    "    if \"subtype\" in df.columns:\n",
    "        df[\"subtype\"] = df[\"subtype\"].str.upper().str.strip().str.replace(\" \", \"\")\n",
    "\n",
    "    # Clean location data - REMOVE ALL SPACES\n",
    "    location_cols = [\"province\", \"locality\"]\n",
    "    for col in location_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.strip().str.title().str.replace(\" \", \"\")\n",
    "            print(f\"Unique {col}s: {df[col].nunique()}\")\n",
    "\n",
    "    # Clean ALL text columns - remove spaces everywhere\n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for col in text_cols:\n",
    "        if col not in [\"url\"]:  # Keep URLs as they are\n",
    "            df[col] = df[col].astype(str).str.strip().str.replace(\" \", \"\")\n",
    "            print(f\"Removed spaces from {col}\")\n",
    "            \n",
    "        \n",
    "    # Step 7: Handle boolean columns (updated list without removed columns)\n",
    "    print(\"\\n Step 7: Cleaning boolean features...\")\n",
    "\n",
    "    boolean_cols = [\n",
    "        \"hasAttic\",\n",
    "        \"hasBasement\",\n",
    "        \"hasDressingRoom\",\n",
    "        \"hasDiningRoom\",\n",
    "        \"hasLift\",\n",
    "        \"hasHeatPump\",\n",
    "        \"hasPhotovoltaicPanels\",\n",
    "        \"hasTerrace\",\n",
    "        \"hasAirConditioning\",\n",
    "        \"hasArmoredDoor\",\n",
    "        \"hasVisiophone\",\n",
    "        \"hasOffice\",\n",
    "        \"hasSwimmingPool\",\n",
    "        \"hasFireplace\",\n",
    "        \"hasLivingRoom\",\n",
    "        \"hasGarden\" \n",
    "    ]\n",
    "\n",
    "    for col in boolean_cols:\n",
    "        if col in df.columns:\n",
    "            # Convert to proper boolean (True/False/NaN)\n",
    "            df[col] = df[col].map(\n",
    "                {True: True, False: False, \"True\": True, \"False\": False}\n",
    "            )\n",
    "            #df[col] = df[col].fillna(False)  # Convert any remaining NaN to False\n",
    "            #print(f\"{col}: {df[col].value_counts(dropna=False).to_dict()}\")\n",
    "\n",
    "    # Step 8: Clean energy performance (EPC)\n",
    "    print(\"\\n Step 8: Cleaning energy performance...\")\n",
    "    if \"epcScore\" in df.columns:\n",
    "        epc_scores={\"A++\":\"A\",\"A+\":\"A\",\"A\":\"A\",\"B\":\"B\",\"C\":\"C\",\"D\":\"D\",\"E\":\"E\",\"F\":\"F\",\"G\":\"G\",\"G_C\":\"G\",\"F_D\":\"F\",\"E_C\":\"E\",\"C_B\":\"C\",\"E_D\":\"E\",\"F_C\":\"F\",\"C_A\":\"C\",\"G_F\":\"G\",\"G_E\":\"G\",\"D_C\":\"D\"}\n",
    "        df[\"epcScore\"]=df[\"epcScore\"].map(epc_scores)\n",
    "    #if \"epcScore\" in df.columns:\n",
    "        '''valid_epc = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "        df.loc[~df[\"epcScore\"].isin(valid_epc), \"epcScore\"] = np.nan\n",
    "        print(f\"EPC distribution: {df['epcScore'].value_counts().to_dict()}\")'''\n",
    "        \n",
    "    # Step 8.1 : fill nulls in bedroom count with value in room count\n",
    "     \n",
    "   # if 'bedroomCount' in df.columns and 'roomCount' in df.columns:\n",
    "        # fill bedroom count to 1 if room count is not null\n",
    "      #  df.loc[df['roomCount'].notna() & df['bedroomCount'].isna(), 'bedroomCount'] = 1\n",
    "\n",
    "    # Step 9: Remove columns with too many missing values\n",
    "    print(\"\\n Step 9: Removing columns with too many missing values...\")\n",
    "\n",
    "    # Remove columns where more than 80% of values are missing\n",
    "    threshold = 0.5\n",
    "    missing_pct = df.isnull().sum() / len(df)\n",
    "    cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        print(f\"Removed columns with >80% missing: {cols_to_drop}\")\n",
    "\n",
    "    # Step 10: Final summary\n",
    "    print(\"\\n Data cleaning completed!\")\n",
    "    print(f\"Final data shape: {df.shape}\")\n",
    "    print(f\"Columns kept: {list(df.columns)}\")\n",
    "\n",
    "    # Show missing values summary\n",
    "    print(\"\\n Missing values summary:\")\n",
    "    missing_summary = df.isnull().sum()\n",
    "    missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "    for col, missing_count in missing_summary.items():\n",
    "        pct = (missing_count / len(df)) * 100\n",
    "        print(f\"  {col}: {missing_count} ({pct:.1f}%)\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8da805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting data cleaning...\n",
      "Original data shape: (80368, 53)\n",
      "\n",
      " Step 1: Removing empty rows and columns...\n",
      "After removing empty: (80368, 49)\n",
      "\n",
      " Step 2: Cleaning column names...\n",
      "Removed 'Unnamed: 0' column\n",
      "\n",
      " Step 2.1: Removing specified columns...\n",
      "Removed specified columns: ['heatingType', 'hasThermicPanels', 'gardenOrientation', 'terraceOrientation', 'roomCount', 'diningRoomSurface', 'streetFacadeWidth', 'kitchenSurface']\n",
      "\n",
      " Step 2.2: Merging parking columns...\n",
      "Merged parking columns into 'totalParkingCount': {0.0: 44853, 1.0: 17610, 2.0: 7457, 3.0: 3308, 4.0: 2356, 5.0: 1402, 6.0: 1060, 7.0: 568, 8.0: 369, 10.0: 238, 9.0: 167, 11.0: 145, 12.0: 126, 13.0: 70, 14.0: 54, 15.0: 51, 20.0: 48, 16.0: 43, 17.0: 34, 30.0: 28, 18.0: 28, 22.0: 24, 50.0: 17, 23.0: 16, 79.0: 14, 21.0: 13, 25.0: 13, 24.0: 13, 19.0: 13, 36.0: 13, 32.0: 12, 40.0: 11, 28.0: 10, 27.0: 10, 100.0: 9, 26.0: 9, 29.0: 9, 117.0: 8, 34.0: 8, 67.0: 5, 35.0: 5, 60.0: 4, 109.0: 4, 31.0: 4, 87.0: 4, 53.0: 4, 44.0: 4, 42.0: 3, 120.0: 3, 33.0: 3, 64.0: 3, 57.0: 3, 75.0: 3, 51.0: 3, 70.0: 3, 160.0: 3, 41.0: 3, 38.0: 3, 49.0: 3, 88.0: 3, 113.0: 3, 46.0: 3, 123.0: 3, 202.0: 2, 101.0: 2, 81.0: 2, 39.0: 2, 48.0: 2, 77.0: 2, 59.0: 2, 47.0: 2, 63.0: 2, 55.0: 2, 133.0: 2, 58.0: 2, 52.0: 2, 135.0: 1, 130.0: 1, 164.0: 1, 400.0: 1, 20000.0: 1, 56.0: 1, 300.0: 1, 96.0: 1, 50000.0: 1, 1885.0: 1, 263.0: 1, 17000.0: 1, 68.0: 1, 2007.0: 1, 45000.0: 1, 83.0: 1, 110.0: 1, 101015.0: 1, 43.0: 1, 37.0: 1, 62.0: 1, 80.0: 1, 192.0: 1}\n",
      "\n",
      " Step 3: Removing duplicate properties...\n",
      "Removed 1 duplicate properties\n",
      "\n",
      " Step 4: Cleaning price data...\n",
      "Removed 3998 rows with missing prices\n",
      "\n",
      " Step 4.1: Handling garden surface imputation...\n",
      "Imputed 0 gardenSurface values with 0 where hasGarden=False\n",
      "Garden surface stats: min=1.0, max=734674.0, median=171.0\n",
      "\n",
      " Step 5: Cleaning numeric features...\n",
      "Cleaned bedroomCount: 73168 valid values\n",
      "Cleaned bathroomCount: 66578 valid values\n",
      "Cleaned toiletCount: 54411 valid values\n",
      "Cleaned terraceSurface: 28598 valid values\n",
      "Cleaned totalParkingCount: 75546 valid values\n",
      "\n",
      " Step 6: Cleaning categorical features...\n",
      "Property types: {'HOUSE': 45227, 'APARTMENT': 31142}\n",
      "Unique provinces: 11\n",
      "Unique localitys: 3824\n",
      "Removed spaces from type\n",
      "Removed spaces from subtype\n",
      "Removed spaces from province\n",
      "Removed spaces from locality\n",
      "Removed spaces from hasAttic\n",
      "Removed spaces from hasBasement\n",
      "Removed spaces from hasDressingRoom\n",
      "Removed spaces from hasDiningRoom\n",
      "Removed spaces from buildingCondition\n",
      "Removed spaces from hasLift\n",
      "Removed spaces from floodZoneType\n",
      "Removed spaces from hasHeatPump\n",
      "Removed spaces from hasPhotovoltaicPanels\n",
      "Removed spaces from kitchenType\n",
      "Removed spaces from hasLivingRoom\n",
      "Removed spaces from hasGarden\n",
      "Removed spaces from hasAirConditioning\n",
      "Removed spaces from hasArmoredDoor\n",
      "Removed spaces from hasVisiophone\n",
      "Removed spaces from hasOffice\n",
      "Removed spaces from hasSwimmingPool\n",
      "Removed spaces from hasFireplace\n",
      "Removed spaces from hasTerrace\n",
      "Removed spaces from epcScore\n",
      "\n",
      " Step 7: Cleaning boolean features...\n",
      "\n",
      " Step 8: Cleaning energy performance...\n",
      "\n",
      " Step 9: Removing columns with too many missing values...\n",
      "Removed columns with >80% missing: ['hasAttic', 'hasBasement', 'hasDressingRoom', 'hasDiningRoom', 'floorCount', 'hasLift', 'hasHeatPump', 'hasPhotovoltaicPanels', 'livingRoomSurface', 'hasGarden', 'gardenSurface', 'hasAirConditioning', 'hasArmoredDoor', 'hasVisiophone', 'hasOffice', 'hasSwimmingPool', 'hasFireplace', 'terraceSurface']\n",
      "\n",
      " Data cleaning completed!\n",
      "Final data shape: (76369, 21)\n",
      "Columns kept: ['id', 'type', 'subtype', 'bedroomCount', 'bathroomCount', 'province', 'locality', 'postCode', 'habitableSurface', 'buildingCondition', 'buildingConstructionYear', 'facedeCount', 'floodZoneType', 'kitchenType', 'landSurface', 'hasLivingRoom', 'toiletCount', 'hasTerrace', 'epcScore', 'price', 'totalParkingCount']\n",
      "\n",
      " Missing values summary:\n",
      "  landSurface: 36833 (48.2%)\n",
      "  hasLivingRoom: 33500 (43.9%)\n",
      "  hasTerrace: 28943 (37.9%)\n",
      "  buildingConstructionYear: 27268 (35.7%)\n",
      "  facedeCount: 23136 (30.3%)\n",
      "  toiletCount: 21958 (28.8%)\n",
      "  epcScore: 11967 (15.7%)\n",
      "  bathroomCount: 9791 (12.8%)\n",
      "  habitableSurface: 8590 (11.2%)\n",
      "  bedroomCount: 3201 (4.2%)\n",
      "  totalParkingCount: 823 (1.1%)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Load your data:\n",
    "    df = pd.read_csv(\"data/final_dataset.csv\")\n",
    "\n",
    "    # 2. Clean the data:\n",
    "    df_clean = clean_real_estate_data(df)\n",
    "\n",
    "    # 3. Apply categorical encoding:\n",
    "   # df_encoded, encoding_mappings = full_encoding_pipeline(df_clean)\n",
    "\n",
    "    # 4. Save the results:\n",
    "    df_clean.to_csv(\"data/cleaned_data1.csv\", index=False)\n",
    "    #df_encoded.to_csv(\"ml_ready_real_estate_data.csv\", index=False)\n",
    "\n",
    "    #print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    #print(f\"Cleaned data saved: cleaned_data1.csv ({df_clean.shape})\")\n",
    "    #print(f\"ML-ready data saved: ml_ready_real_estate_data.csv ({df_encoded.shape})\")\n",
    "    #print(f\"Encoding mappings: {list(encoding_mappings.keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

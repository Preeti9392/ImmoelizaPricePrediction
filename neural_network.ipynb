{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b906f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# --- Import TensorFlow and Keras ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c371af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'data/cleaned_data_after_imputation.csv' not found.\n",
      "Please ensure the CSV file is in the 'data' directory relative to your script.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 3. One-Hot Encode Categorical Columns\u001b[39;00m\n\u001b[0;32m     13\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m one_hot_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(df[categorical_columns])\n\u001b[0;32m     15\u001b[0m one_hot_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(one_hot_encoded, columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_columns))\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, one_hot_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"data/nonull_smalldataset.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/cleaned_data_after_imputation.csv' not found.\")\n",
    "    print(\"Please ensure the CSV file is in the 'data' directory relative to your script.\")\n",
    "    exit() # Exit if the file isn't found to prevent further errors\n",
    "\n",
    "# 2. Define Column Types\n",
    "numeric_columns = [\"bedroomCount\", \"toilet_and_bath\", \"habitableSurface\", \"facedeCount\", \"hasTerrace\", \"totalParkingCount\"]\n",
    "categorical_columns = [\"type\", \"subtype\", \"province\", \"locality\", \"postCode\", \"buildingCondition\", \"epcScore\"]\n",
    "\n",
    "# 3. One-Hot Encode Categorical Columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "df = pd.concat([df, one_hot_df], axis=1)\n",
    "df = df.drop(categorical_columns, axis=1)\n",
    "\n",
    "# 4. Separate Features (X) and Target (y)\n",
    "X = df.drop(columns=\"price\")\n",
    "y = df[\"price\"]\n",
    "\n",
    "# 5. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# 6. Scale Numerical Features (Crucial for Neural Networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Neural Network Model Definition and Training ---\n",
    "\n",
    "# 1. Determine Input Shape\n",
    "# This is the number of features your scaled X_train has.\n",
    "input_features = X_train_scaled.shape[1]\n",
    "\n",
    "# 2. Build the Neural Network Architecture\n",
    "# We'll use a Sequential model (a linear stack of layers).\n",
    "nn_model = keras.Sequential([\n",
    "    # Input Layer + First Hidden Layer\n",
    "    # 'units' is the number of neurons in this layer.\n",
    "    # 'activation' is the activation function (e.g., 'relu' for hidden layers).\n",
    "    # 'input_shape' is only required for the very first layer.\n",
    "    layers.Dense(units=128, activation='relu', input_shape=(input_features,)),\n",
    "\n",
    "    # Second Hidden Layer\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "\n",
    "    # Third Hidden Layer (optional, you can add more or fewer layers)\n",
    "    layers.Dense(units=32, activation='relu'),\n",
    "\n",
    "    # Output Layer for Regression\n",
    "    # 'units=1' because we are predicting a single continuous 'price' value.\n",
    "    # 'activation='linear'' (or simply omitting 'activation') for regression,\n",
    "    # as we don't want to constrain the output range.\n",
    "    layers.Dense(units=1, activation='linear')\n",
    "])\n",
    "\n",
    "# 3. Compile the Model\n",
    "# This configures the learning process.\n",
    "nn_model.compile(\n",
    "    optimizer='adam',      # Adam is a popular and generally effective optimizer.\n",
    "    loss='mean_squared_error', # Mean Squared Error is the standard loss for regression.\n",
    "    metrics=['mse', 'mae'] # Metrics to track during training (MSE and Mean Absolute Error).\n",
    ")\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "print(\"--- Neural Network Model Summary ---\")\n",
    "nn_model.summary()\n",
    "\n",
    "# 4. Train the Neural Network\n",
    "# 'epochs': How many times to iterate over the entire training dataset.\n",
    "# 'batch_size': Number of samples processed before updating weights.\n",
    "# 'validation_split': Sets aside a portion of the training data for validation during training.\n",
    "# 'verbose': Controls what is printed during training (0=silent, 1=progress bar, 2=one line per epoch).\n",
    "print(\"\\n--- Training Neural Network ---\")\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=150,           # You can increase or decrease this\n",
    "    batch_size=32,        # Common batch size, experiment with 16, 64, etc.\n",
    "    validation_split=0.1, # Use 10% of training data for validation\n",
    "    verbose=1             # Show training progress\n",
    ")\n",
    "\n",
    "# 5. Evaluate the Neural Network on the Test Set\n",
    "print(\"\\n--- Neural Network Evaluation on Test Set ---\")\n",
    "# evaluate returns the loss and any metrics specified in compile()\n",
    "nn_loss, nn_mse_test, nn_mae_test = nn_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Neural Network Test Loss (MSE): {nn_mse_test:.4f}\")\n",
    "print(f\"Neural Network Test MAE: {nn_mae_test:.4f}\")\n",
    "\n",
    "# Calculate R-squared for test set\n",
    "nn_predictions_test = nn_model.predict(X_test_scaled)\n",
    "nn_r2_test = r2_score(y_test, nn_predictions_test)\n",
    "print(f\"Neural Network Test R-squared: {nn_r2_test:.4f}\")\n",
    "\n",
    "# 6. Evaluate on Training Set (Optional - helps identify overfitting)\n",
    "print(\"\\n--- Neural Network Evaluation on Training Set ---\")\n",
    "nn_predictions_train = nn_model.predict(X_train_scaled)\n",
    "nn_r2_train = r2_score(y_train, nn_predictions_train)\n",
    "print(f\"Neural Network Train R-squared: {nn_r2_train:.4f}\")\n",
    "\n",
    "# --- Optional: Visualize Training History ---\n",
    "# (Requires matplotlib)\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# plt.figure(figsize=(12, 5))\n",
    "#\n",
    "# # Plot training & validation loss values\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss (MSE)')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "#\n",
    "# # Plot training & validation MAE values\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['mae'])\n",
    "# plt.plot(history.history['val_mae'])\n",
    "# plt.title('Model MAE')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
